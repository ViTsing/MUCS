llamafactory-cli train --stage sft --do_train --model_name_or_path /qwen2.5-7b --dataset tulu3_train_0 --dataset_dir ./data --template qwen --finetuning_type lora --output_dir ./saves/qwen2.5-7b/tulu3_train_0 --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --warmup_steps 60 --learning_rate 5e-6 --num_train_epochs 5.0 --fp16

llamafactory-cli train --stage sft --do_train --model_name_or_path /llama-1-7b --dataset alpaca_train_0 --dataset_dir ./data --template alpaca --finetuning_type lora --output_dir ./saves/llama-1-7b/alpaca_train_0 --overwrite_cache --overwrite_output_dir --cutoff_len 512 --preprocessing_num_workers 16 --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --gradient_accumulation_steps 64 --lr_scheduler_type cosine --warmup_ratio 0.03 --learning_rate 2e-5 --num_train_epochs 3.0 --fp16